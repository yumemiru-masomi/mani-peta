// src/app/api/sensitive-texts/route.ts
import { NextResponse } from "next/server";
import vision from "@google-cloud/vision";

import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";
import { createCanvas, loadImage } from "@napi-rs/canvas";
import { writeFile } from "fs/promises";

export const config = {
  api: {
    // Next.jsãŒãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æœ¬æ–‡ã‚’è‡ªå‹•è§£æžã™ã‚‹æ©Ÿæ§‹ã‚’ç„¡åŠ¹åŒ–
    bodyParser: false,
  },
};

// AIã«è¿”ã™ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¹ã‚­ãƒ¼ãƒžã‚’å®šç¾©
const schema = {
  description: "List of sensitive texts", // ã‚¹ã‚­ãƒ¼ãƒžã®èª¬æ˜Ž
  type: SchemaType.OBJECT, // ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåž‹ã§å®šç¾©
  properties: {
    sensitiveTexts: {
      type: SchemaType.ARRAY, // é…åˆ—åž‹ã¨ã—ã¦å®šç¾©
      description: "Array of sensitive text strings", // é…åˆ—ã®èª¬æ˜Ž
      items: {
        type: SchemaType.STRING, // é…åˆ—è¦ç´ ã¯æ–‡å­—åˆ—åž‹
      },
    },
  },
  required: ["sensitiveTexts"], // å¿…é ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æŒ‡å®š
};

// POSTãƒ¡ã‚½ãƒƒãƒ‰ã®å‡¦ç†
export async function POST(request: Request) {
  // Google Vision API ã¨ Generative AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
  const visionClient = new vision.ImageAnnotatorClient();
  const apiKey = process.env.GOOGLE_GENERATIVE_AI;
  if (!apiKey) {
    throw new Error("Environment variable GOOGLE_GENERATIVE_AI is not set.");
  }

  const generativeAIClient = new GoogleGenerativeAI(apiKey);

  try {
    // Content-Typeã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’è¡¨ã™ãƒ˜ãƒƒãƒ€ãƒ¼
    // Content-TypeãŒãªã„å ´åˆã¯ã€400ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
    const contentType = request.headers.get("content-type");
    if (!contentType) {
      return NextResponse.json(
        { error: "No content-type header" },
        { status: 400 }
      );
    }
    console.log("ðŸ©µ Content-Type:", contentType);

    // ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    const formData = await request.formData();
    const file = formData.get("file") as File;
    const maskText = formData.get("maskText")?.toString();

    if (!file || !maskText) {
      return NextResponse.json(
        { error: "No file uploaded. Please upload an image." },
        { status: 400 }
      );
    }

    console.log("ðŸ§¡ File received:", file);
    console.log("ðŸ’š Mask Text received:", maskText);

    // ç”»åƒã‚’Google Vision APIã«é€ä¿¡
    const fileBuffer = Buffer.from(await file.arrayBuffer());
    const [detectionResult] = await visionClient.textDetection(fileBuffer);
    const textAnnotations = detectionResult.textAnnotations;

    console.log("fileBufferðŸŒŸ", fileBuffer);
    console.log("textAnnotationsðŸ’«", textAnnotations);

    if (!textAnnotations || textAnnotations.length === 0) {
      return NextResponse.json(
        { message: "No text detected in the image." },
        { status: 200 }
      );
    }

    // æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’é…åˆ—ã«å¤‰æ›
    const detectedTexts = textAnnotations
      .slice(1) // æœ€åˆã®è¦ç´ ã¯ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
      .map((annotation) => annotation.description);

    console.log("Extracted Text ArrayðŸ¥°:", detectedTexts);

    // æ¤œå‡ºãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ³ãƒžåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ›
    const textContext = detectedTexts.join(", ");

    // Generative AI ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å½¢å¼ã‚’æŒ‡å®š
    const generativeModel = generativeAIClient.getGenerativeModel({
      model: "models/gemini-1.5-pro", // ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
      generationConfig: {
        responseMimeType: "application/json", // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ JSON å½¢å¼ã§å–å¾—
        responseSchema: schema, // ã‚¹ã‚­ãƒ¼ãƒžã‚’è¨­å®š
      },
    });

    // AIã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    const prompt = `ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€ãƒ–ãƒ­ã‚°å…¬é–‹æ™‚ã«éš ã—ãŸã»ã†ãŒè‰¯ã„æƒ…å ±ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚éš ã™ã¹ãæƒ…å ±ã®åŸºæº–ã¯æ¬¡ã®é€šã‚Šã§ã™ï¼š
  1. APIã‚­ãƒ¼ï¼šã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨æ•°å­—ãŒæ··åœ¨ã™ã‚‹é•·ã„æ–‡å­—åˆ—ã€‚
  2. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼š'@' ã‚’å«ã‚€æ–‡å­—åˆ—ã€‚
  3. é›»è©±ç•ªå·ï¼šæ•°å­—ã¨ '-' ãŒå«ã¾ã‚Œã‚‹å½¢å¼ï¼ˆä¾‹: "03-1234-5678").
  4. ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ç•ªå·ï¼š16æ¡ã®æ•°å­—ã€‚
  5. å€‹äººåï¼šæ˜Žã‚‰ã‹ã«åå‰ã¨åˆ†ã‹ã‚‹ã‚‚ã®ã€‚
  6. ä¼æ¥­åã‚„ã‚µãƒ¼ãƒ“ã‚¹åï¼šæ¬¡ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã¤ã‚‚ã®ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š
    - é€šå¸¸1ã¤ã¾ãŸã¯è¤‡æ•°ã®å˜èªžã§æ§‹æˆã•ã‚Œã€è¨˜å·ï¼ˆä¾‹: '-', '.'ï¼‰ã‚„ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãŒæ··åœ¨ã™ã‚‹å ´åˆãŒå¤šã„ã€‚
    - æ–‡è„ˆã«åŸºã¥ã„ã¦ã€Œç‰¹å®šã®ãƒ–ãƒ©ãƒ³ãƒ‰ã€ã€Œä¼šç¤¾åã€ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€ãªã©ã¨æŽ¨æ¸¬ã•ã‚Œã‚‹ã‚‚ã®ã€‚
    - ä¸€èˆ¬çš„ãªå˜èªžï¼ˆä¾‹: 'project', 'dashboard', 'add'ï¼‰ã¯å«ã¾ãªã„ã€‚
  7. ä»¥ä¸‹ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€ãã‚Œã¯éš ã™ã¹ãæƒ…å ±ã¨ã—ã¦ãã ã•ã„ã€‚
    - yumemi

  ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€éš ã™ã¹ãæƒ…å ±ã‚’ JSON å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
  å½¢å¼: { "sensitiveTexts": ["éš ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆ1", "éš ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆ2"] }
  text context: ${textContext}
  ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚éš ã™ã¹ãæƒ…å ±ã¨ã—ã¦èªè­˜ã—ã¦ãã ã•ã„: "${maskText}"`;

    // Generative AI ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã¦çµæžœã‚’å–å¾—
    const aiResponse = await generativeModel.generateContent(prompt);
    const responseText = aiResponse.response.text();
    console.log("AI Response:", responseText);

    // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONã¨ã—ã¦è§£æžã—ã€sensitiveTextsã‚’å–å¾—
    const { sensitiveTexts } = JSON.parse(responseText);
    console.log("Sensitive Texts to Mask:ðŸ”¥", sensitiveTexts);

    // éš ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    await writeFile(
      "sensitiveTexts.json",
      JSON.stringify(sensitiveTexts, null, 2)
    );

    // ç”»åƒã‚’èª­ã¿è¾¼ã¿
    const image = await loadImage(fileBuffer);
    const imageWidth = image.width;
    const imageHeight = image.height;

    // Canvasã‚’ä½œæˆã—ã€ç”»åƒã‚’æç”»
    const canvas = createCanvas(imageWidth, imageHeight);
    const context = canvas.getContext("2d");

    context.drawImage(image, 0, 0, imageWidth, imageHeight);

    // ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒžã‚¹ã‚¯å¯¾è±¡ã®å˜èªžã‚’å–å¾—
    const targetWord = process.argv[2]?.toLowerCase();

    // æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ã¦ã€ãƒžã‚¹ã‚¯å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èµ¤ã„é•·æ–¹å½¢ã§éš ã™
    textAnnotations.forEach((annotation, index) => {
      if (
        index !== 0 &&
        annotation.description &&
        sensitiveTexts.includes(annotation.description) &&
        annotation.description.toLowerCase() !== targetWord
      ) {
        console.log(`Text to mask: ${annotation.description}`);

        // ãƒ†ã‚­ã‚¹ãƒˆã‚’å›²ã‚€åº§æ¨™ã‚’å–å¾—
        const vertices =
          annotation.boundingPoly && annotation.boundingPoly.vertices;
        const coordinates = (vertices || []).map((vertex) => ({
          x: vertex.x || 0,
          y: vertex.y || 0,
        }));

        // èµ¤ã„é•·æ–¹å½¢ã‚’å¡—ã‚Šã¤ã¶ã™
        context.fillStyle = "red";
        context.beginPath();
        context.moveTo(coordinates[0].x, coordinates[0].y);
        coordinates.forEach((vertex, i) => {
          if (i > 0) context.lineTo(vertex.x, vertex.y);
        });
        context.closePath();
        context.fill();

        // èµ¤ã„é•·æ–¹å½¢ã®å¤–æž ã‚’æç”»
        context.strokeStyle = "red";
        context.lineWidth = 2;
        context.beginPath();
        context.moveTo(coordinates[0].x, coordinates[0].y);
        coordinates.forEach((vertex, i) => {
          if (i > 0) context.lineTo(vertex.x, vertex.y);
        });
        context.closePath();
        context.stroke();
      }
    });

    const buffer = canvas.toBuffer("image/png");
    return new Response(buffer, {
      headers: {
        "Content-Type": "image/png",
      },
    });
  } catch (error) {
    console.error("Error processing image:", error);
    return NextResponse.json(
      { error: "An error occurred while processing the image." },
      { status: 500 }
    );
  }
}
